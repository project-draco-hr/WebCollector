{
  conf=NutchConfiguration.create();
  fs=FileSystem.get(conf);
  testDir=new Path(conf.get("hadoop.tmp.dir"),"merge-" + System.currentTimeMillis());
  seg1=new Path(testDir,"seg1");
  seg2=new Path(testDir,"seg2");
  out=new Path(testDir,"out");
  System.err.println("Creating large segment 1...");
  DecimalFormat df=new DecimalFormat("0000000");
  Text k=new Text();
  Path ptPath=new Path(new Path(seg1,ParseText.DIR_NAME),"part-00000");
  MapFile.Writer w=new MapFile.Writer(conf,fs,ptPath.toString(),Text.class,ParseText.class);
  long curSize=0;
  countSeg1=0;
  FileStatus fileStatus=fs.getFileStatus(ptPath);
  long blkSize=fileStatus.getBlockSize();
  while (curSize < blkSize * 2) {
    k.set("seg1-" + df.format(countSeg1));
    w.append(k,new ParseText("seg1 text " + countSeg1));
    countSeg1++;
    curSize+=40;
  }
  w.close();
  System.err.println(" - done: " + countSeg1 + " records.");
  System.err.println("Creating large segment 2...");
  ptPath=new Path(new Path(seg2,ParseText.DIR_NAME),"part-00000");
  w=new MapFile.Writer(conf,fs,ptPath.toString(),Text.class,ParseText.class);
  curSize=0;
  countSeg2=0;
  while (curSize < blkSize * 2) {
    k.set("seg2-" + df.format(countSeg2));
    w.append(k,new ParseText("seg2 text " + countSeg2));
    countSeg2++;
    curSize+=40;
  }
  w.close();
  System.err.println(" - done: " + countSeg2 + " records.");
}
