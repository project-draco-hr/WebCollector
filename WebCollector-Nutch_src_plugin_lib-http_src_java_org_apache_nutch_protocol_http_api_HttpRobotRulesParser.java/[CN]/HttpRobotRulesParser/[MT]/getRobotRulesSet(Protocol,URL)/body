{
  String cacheKey=getCacheKey(url);
  BaseRobotRules robotRules=(SimpleRobotRules)CACHE.get(cacheKey);
  boolean cacheRule=true;
  if (robotRules == null) {
    URL redir=null;
    if (LOG.isTraceEnabled()) {
      LOG.trace("cache miss " + url);
    }
    try {
      Response response=((HttpBase)http).getResponse(new URL(url,"/robots.txt"),new CrawlDatum(),true);
      if (response.getCode() == 301 || response.getCode() == 302) {
        String redirection=response.getHeader("Location");
        if (redirection == null) {
          redirection=response.getHeader("location");
        }
        if (redirection != null) {
          if (!redirection.startsWith("http")) {
            redir=new URL(url,redirection);
          }
 else {
            redir=new URL(redirection);
          }
          response=((HttpBase)http).getResponse(redir,new CrawlDatum(),true);
        }
      }
      if (response.getCode() == 200)       robotRules=parseRules(url.toString(),response.getContent(),response.getHeader("Content-Type"),agentNames);
 else       if ((response.getCode() == 403) && (!allowForbidden))       robotRules=FORBID_ALL_RULES;
 else       if (response.getCode() >= 500) {
        cacheRule=false;
        robotRules=EMPTY_RULES;
      }
 else       robotRules=EMPTY_RULES;
    }
 catch (    Throwable t) {
      if (LOG.isInfoEnabled()) {
        LOG.info("Couldn't get robots.txt for " + url + ": "+ t.toString());
      }
      cacheRule=false;
      robotRules=EMPTY_RULES;
    }
    if (cacheRule) {
      CACHE.put(cacheKey,robotRules);
      if (redir != null && !redir.getHost().equalsIgnoreCase(url.getHost())) {
        CACHE.put(getCacheKey(redir),robotRules);
      }
    }
  }
  return robotRules;
}
