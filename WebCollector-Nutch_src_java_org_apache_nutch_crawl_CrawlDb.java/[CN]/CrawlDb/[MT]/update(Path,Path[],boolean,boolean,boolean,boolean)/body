{
  FileSystem fs=FileSystem.get(getConf());
  Path lock=new Path(crawlDb,LOCK_NAME);
  LockUtil.createLockFile(fs,lock,force);
  SimpleDateFormat sdf=new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
  long start=System.currentTimeMillis();
  JobConf job=CrawlDb.createJob(getConf(),crawlDb);
  job.setBoolean(CRAWLDB_ADDITIONS_ALLOWED,additionsAllowed);
  job.setBoolean(CrawlDbFilter.URL_FILTERING,filter);
  job.setBoolean(CrawlDbFilter.URL_NORMALIZING,normalize);
  boolean url404Purging=job.getBoolean(CRAWLDB_PURGE_404,false);
  if (LOG.isInfoEnabled()) {
    LOG.info("CrawlDb update: starting at " + sdf.format(start));
    LOG.info("CrawlDb update: db: " + crawlDb);
    LOG.info("CrawlDb update: segments: " + Arrays.asList(segments));
    LOG.info("CrawlDb update: additions allowed: " + additionsAllowed);
    LOG.info("CrawlDb update: URL normalizing: " + normalize);
    LOG.info("CrawlDb update: URL filtering: " + filter);
    LOG.info("CrawlDb update: 404 purging: " + url404Purging);
  }
  for (int i=0; i < segments.length; i++) {
    Path fetch=new Path(segments[i],CrawlDatum.FETCH_DIR_NAME);
    Path parse=new Path(segments[i],CrawlDatum.PARSE_DIR_NAME);
    if (fs.exists(fetch) && fs.exists(parse)) {
      FileInputFormat.addInputPath(job,fetch);
      FileInputFormat.addInputPath(job,parse);
    }
 else {
      LOG.info(" - skipping invalid segment " + segments[i]);
    }
  }
  if (LOG.isInfoEnabled()) {
    LOG.info("CrawlDb update: Merging segment data into db.");
  }
  try {
    JobClient.runJob(job);
  }
 catch (  IOException e) {
    LockUtil.removeLockFile(fs,lock);
    Path outPath=FileOutputFormat.getOutputPath(job);
    if (fs.exists(outPath))     fs.delete(outPath,true);
    throw e;
  }
  CrawlDb.install(job,crawlDb);
  long end=System.currentTimeMillis();
  LOG.info("CrawlDb update: finished at " + sdf.format(end) + ", elapsed: "+ TimingUtil.elapsedTime(start,end));
}
